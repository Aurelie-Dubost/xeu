{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b14fe92c-96b5-4f0e-be21-a9ab93f6557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install schedule\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import schedule\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from vol_data import *\n",
    "from surface import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823ebe4-75c8-4e75-bccb-48099e31e6c6",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b42ccb4-aec5-4f8c-8712-2d42bc2aedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameters\n",
    "spe_time = \"22:48\"\n",
    "\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2024-05-28'\n",
    "\n",
    "udl_list = ['JP_NKY', 'DE_DAX', 'GB_FTSE100', 'CH_SMI', 'IT_FTMIB', 'ES_IBEX', 'US_SPX', 'EU_STOXX50E', 'EU_SX7E', 'EU_SX7P',\n",
    "            'EU_SXDP', 'US_KO', 'US_MCD', 'US_KOMO', 'EU_SXPP', 'EU_SOXP', 'HK_HSI']\n",
    "\n",
    "matu_list = [1, 3, 6, 9, 12, 18, 24, 36]\n",
    "moneyness_list = [120, 110, 105, 102.5, 100, 97.5, 95, 90, 80]\n",
    "delta_list = [5, 10, 15, 20, 25, 35, 50, 65, 75, 90, 95]\n",
    "\n",
    "path = \"vol_surf.pickle\"  # Path to save the pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2181ef8f-6c48-478d-8c8a-0e0ba573c10b",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1864181d-d298-4840-9f92-ef85bc273154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vol_moneyness(udl_list, matu_list, moneyness_list, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Generate random volatility moneyness data.\n",
    "    \"\"\"\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='B')\n",
    "    data = {\n",
    "        (udl, matu, mon): np.random.rand(len(date_range)) * 100\n",
    "        for udl in udl_list\n",
    "        for matu in matu_list\n",
    "        for mon in moneyness_list\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=date_range)\n",
    "    df.columns = pd.MultiIndex.from_tuples(df.columns, names=['udl', 'matu', 'moneyness'])\n",
    "    return df\n",
    "\n",
    "def get_vol_delta(udl_list, matu_list, delta_list, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Generate random volatility delta data.\n",
    "    \"\"\"\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='B')\n",
    "    data = {\n",
    "        (udl, matu, delta): np.random.rand(len(date_range)) * 100\n",
    "        for udl in udl_list\n",
    "        for matu in matu_list\n",
    "        for delta in delta_list\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=date_range)\n",
    "    df.columns = pd.MultiIndex.from_tuples(df.columns, names=['udl', 'matu', 'delta'])\n",
    "    return df\n",
    "    \n",
    "def generate_table(udl_list, matu_list, moneyness_list, delta_list, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Generate random volatility data for moneyness and delta in a single step and create a structured DataFrame.\n",
    "\n",
    "    Args:\n",
    "        udl_list (list): List of underlying assets.\n",
    "        matu_list (list): List of maturities.\n",
    "        moneyness_list (list): List of moneyness levels.\n",
    "        delta_list (list): List of delta levels.\n",
    "        start_date (str): Start date for the data.\n",
    "        end_date (str): End date for the data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with combined moneyness and delta data, indexed by date with MultiIndex columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='B')\n",
    "\n",
    "        # Generate moneyness and delta data\n",
    "        df_vol_moneyness = get_vol_moneyness(udl_list, matu_list, moneyness_list, start_date, end_date)\n",
    "        df_vol_delta = get_vol_delta(udl_list, matu_list, delta_list, start_date, end_date)\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        # Combine moneyness data\n",
    "        for udl in udl_list:\n",
    "            for matu in matu_list:\n",
    "                for mon in moneyness_list:\n",
    "                    key = (udl, matu, mon)\n",
    "                    data[(udl, 'IV', matu, mon)] = df_vol_moneyness[key]\n",
    "\n",
    "        # Combine delta data\n",
    "        for udl in udl_list:\n",
    "            for matu in matu_list:\n",
    "                for delta in delta_list:\n",
    "                    key = (udl, matu, delta)\n",
    "                    data[(udl, 'IVFD', matu, delta)] = df_vol_delta[key]\n",
    "\n",
    "        df = pd.DataFrame(data, index=date_range)\n",
    "        df.columns = pd.MultiIndex.from_tuples(df.columns, names=['udl', 'param', 'matu', 'value'])\n",
    "        df.index.name = 'Date'\n",
    "        df.ffill(inplace=True)\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during table generation: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def df_to_nested_dict(df):\n",
    "    nested_dict = {}\n",
    "    for date, row in df.iterrows():\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        nested_dict[date_str] = {}\n",
    "        for col, val in row.items():\n",
    "            udl, param, matu, value = col\n",
    "            nested_dict[date_str].setdefault(udl, {}).setdefault(param, {}).setdefault(matu, {})[value] = val\n",
    "    return nested_dict\n",
    "\n",
    "def generate_and_save_vol_surf():\n",
    "    try:\n",
    "        # Generate the table\n",
    "        df = generate_table(udl_list, matu_list, moneyness_list, delta_list, start_date, end_date)\n",
    "\n",
    "        # Convert DataFrame to nested dictionary\n",
    "        nested_dict = df_to_nested_dict(df)\n",
    "\n",
    "        # Save the nested dictionary to a pickle file\n",
    "        with open(path, 'wb') as handle:\n",
    "            pickle.dump(nested_dict, handle)\n",
    "\n",
    "        print(f\"{datetime.now()} - Volatility surface data generated and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def run_schedule():\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)  # Wait a bit before checking again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4332439-d679-4d2d-8ca9-0b684b9d6148",
   "metadata": {},
   "source": [
    "### Exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e537211-e6e6-411a-99bc-9ca8c037e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schedule the job to run at midnight every day\n",
    "schedule.every().day.at(spe_time).do(generate_and_save_vol_surf)\n",
    "\n",
    "# Start the scheduler in a new thread\n",
    "scheduler_thread = threading.Thread(target=run_schedule, daemon=True)\n",
    "scheduler_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8022d798-2f05-441c-8855-ecc815341a6b",
   "metadata": {},
   "source": [
    "### Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7eb6b5a0-325d-460b-b81c-bed70c880fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduler started. The task will run every day at: 22:48\n",
      "2024-05-27 22:48:01.210940 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:01.662461 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:02.050020 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:02.432224 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:03.090510 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:03.546454 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:04.080194 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:04.096843 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:04.787595 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:05.431505 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:06.539297 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:06.701723 - Volatility surface data generated and saved successfully.\n",
      "2024-05-27 22:48:06.996513 - Volatility surface data generated and saved successfully.2024-05-27 22:48:06.996769 - Volatility surface data generated and saved successfully.\n",
      "\n",
      "2024-05-27 22:48:07.079793 - Volatility surface data generated and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Exception in thread Thread-5 (run_schedule):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "Thread-9 (run_schedule):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/y0/5dv_k1x105l2r3xvbg3mwj3h0000gn/T/ipykernel_28713/984865960.py\", line 20, in run_schedule\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/y0/5dv_k1x105l2r3xvbg3mwj3h0000gn/T/ipykernel_28713/984865960.py\", line 20, in run_schedule\n",
      "NameError: name 'time' is not defined\n",
      "NameError: name 'time' is not defined\n"
     ]
    }
   ],
   "source": [
    "print(\"Scheduler started. The task will run every day at: \" + spe_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058383f1-4800-4b5f-a942-8b44e35d7716",
   "metadata": {},
   "source": [
    "### To go further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eae9b7f4-a239-4e8a-8494-9fb8dba34972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames are equal: True\n",
      "DataFrames are equal: True\n"
     ]
    }
   ],
   "source": [
    "# Generate table\n",
    "df = generate_table(udl_list, matu_list, moneyness_list, delta_list, start_date, end_date)\n",
    "\n",
    "# Convert DataFrame to nested dictionary and back to DataFrame\n",
    "nested_dict = df_to_nested_dict(df)\n",
    "reconstructed_df = nested_dict_to_df(nested_dict)\n",
    "\n",
    "# Check if the reconstructed DataFrame is equal to the original\n",
    "comparison = df.equals(reconstructed_df)\n",
    "print(f\"DataFrames are equal: {comparison}\")\n",
    "\n",
    "if not comparison:\n",
    "    print(\"Differences found between original and reconstructed DataFrames:\")\n",
    "    print(\"Original DataFrame head:\")\n",
    "    print(df.head())\n",
    "    print(\"Reconstructed DataFrame head:\")\n",
    "    print(reconstructed_df.head())\n",
    "\n",
    "    print(\"Original DataFrame index and columns:\")\n",
    "    print(df.index)\n",
    "    print(df.columns)\n",
    "\n",
    "    print(\"Reconstructed DataFrame index and columns:\")\n",
    "    print(reconstructed_df.index)\n",
    "    print(reconstructed_df.columns)\n",
    "\n",
    "# SANITY CHECK - Check if the reconstructed DataFrame is equal to the original\n",
    "comparison = df.equals(reconstructed_df)\n",
    "print(f\"DataFrames are equal: {comparison}\")\n",
    "\n",
    "if not comparison:\n",
    "    print(\"Differences found between original and reconstructed DataFrames:\")\n",
    "    print(\"Original DataFrame head:\")\n",
    "    print(df.head())\n",
    "    print(\"Reconstructed DataFrame head:\")\n",
    "    print(reconstructed_df.head())\n",
    "    \n",
    "    print(\"Original DataFrame index and columns:\")\n",
    "    print(df.index)\n",
    "    print(df.columns)\n",
    "    \n",
    "    print(\"Reconstructed DataFrame index and columns:\")\n",
    "    print(reconstructed_df.index)\n",
    "    print(reconstructed_df.columns)\n",
    "\n",
    "# Set initial visibility\n",
    "toggle_date_widgets(surface_type_widget.value)\n",
    "\n",
    "# Display the widgets and output\n",
    "display(main_box)\n",
    "\n",
    "# Function to toggle date widgets visibility\n",
    "def toggle_date_widgets(surface_type):\n",
    "    if surface_type in ['Percentile', 'Z-score']:\n",
    "        start_date_widget.layout.display = 'block'\n",
    "        end_date_widget.layout.display = 'block'\n",
    "        date_widget.layout.display = 'none'\n",
    "    else:\n",
    "        start_date_widget.layout.display = 'none'\n",
    "        end_date_widget.layout.display = 'none'\n",
    "        date_widget.layout.display = 'block'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
